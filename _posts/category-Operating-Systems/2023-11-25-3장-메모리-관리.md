---
title: "3장 메모리 관리"
excerpt: "메인 메모리(main memory)는 매우 조심스럽게 관리되어야 할 중요한 자원이다. 프로그램은 매우 크고 빠른 메모리를 요구한다. 프로그램은 가용한 메모리가 생기는 만큼 크기가 증가한다고 말할 수 있다."

wirter: Myeongwoo Yoon
categories:
  - Operating Systems
tags:
  - Programing

use_math: true
toc: true
toc_sticky: true
 
date: 2023-11-25
last_modified_at: 2023-11-25
---

　모든 프로그래머가 원하는 것은 무한히 크고, 빠르며, 자신이 혼자 사용할 수 있는 메모리이다. 또한 비휘발성(nonvolatile) 메모리를 원한다. 또 한가지 중요한 것은 가격이 비싸지 않아야 한다는 것이다. 하지만 현재 기술로는 위의 모든 요구 조건을 만족시킬 수 있는 메모리를 만들 수는 없다.<br/>
　**메모리 계층 구조(memory hierarchy)**란 캐시 메모리(cache memory), 메인 메모리(main memory), 디스크 스토리지(storage) 등의 서로 다른 특성을 갖는 메모리들을 층 구조로 구성하는 방법이다. 캐시 메모리는 보통 메가바이트(megabyte) 크기를 가지며, 빠르고 휘발성이며 비싸다. 반면 메인 메모리는 가가바이트(gigabyte) 크기를 가지며, 중간 속도이고 휘발성이며 중간 정도의 가격을 갖는다. 마지막으로 디스크 스토리지는 테라바이트(terabyte) 크기를 가지며, 느리고 비휘발성이며 가격이 싸다. 이러한 계층 구조를 사용하기 좋은 모델로 추상화시키고 이 추상화된 객체를 관리하는 것이 운영체제의 역할이다.<br/>
　운영체제 중에서 메모리 계층 구조 관리를 담당하는 부분을 **메모리 관리자(memory manager)**라고 한다. 메모리 관리자는 현재 사용 중인 메모리 부분을 파악하고, 프로세스들이 메모리를 필요로 하면 할당해 주고, 더 이상 사용하지 않으면 해제하는 작업을 실행한다.<br/>

메모리 추상화가 없는 컴퓨터
======
　메모리 추상화의 가장 단순한 형태는 추상화를 사용하지 않는 것이다. 즉 모든 프로그램은 물리 메모리를 직접 사용하였다.(ex. MOV REGISTER1, 1000) 메모리 추상화가 없는 환경에서는 두 개의 프로그램이 동시에 메모리에서 실행된다는 것은 불가능하다.<br/>
　물리 메모리를 직접 사용하는 추상화 없는 메모리 모델에서도 설계에 여러 가지 선택이 가능하다. 다음은 3가지 선택의 경우들을 보여준다.<br/>
<p align="center"><img src="/assets/img/Operating-Systems/3장-메모리-관리/1-1.png" width="600"></p>

　(a)는 RAM(Random Access Memory: 임의접근 메모리)의 아래 부분에 운영체제가 존재하고 그 위에 사용자 프로그램이 존재하는 메모리 구성의 예를 보여준다. 반면 (b)는 주소 공간 상단에 배치된 ROM(Read Only Memory)에 운영체제가 존재하는 예를 보여주며, (c)는 ROM에는 장치 드라이버가 존재하고 그 아래에 있는 RAM에 사용자 프로그램과 운영체제가 존재하는 예를 보여준다.<br/>
　컴퓨터 시스템이 이와 같은 방법으로 구성되어 있으면 대부분 한 순간에 하나의 프로세스만 동작하게 된다. 사용자가 명령을 입력하면 운영체제는 요청한 명령 프로그램을 디스크에서 메모리로 적재하고 그것을 실행한다. 그 프로세스가 종료되면 운영체제는 프롬프트를 출력하고 사용자가 다른 명령을 입력할 때까지 대기한다. 만일 새로운 명령이 요청되면 운영체제는 새로운 명령을 위한 프로그램을 디스크에서 메모리로 적재하는데 이때 기존에 있던 프로그램을 덮어쓰게 된다.<br/>
　메모리 추상화가 없는 시스템에서 병령성(parallelism)을 획득하는 방법 중에 하나는 여러 개의 스레드(multiple thread)를 사용하는 것이다. 한 프로세스에 존재하는 스레드들은 모두 같은 메모리 이미지를 공유하기 때문에 가능하다. 하지만 이것의 다른 의미는 스레드들이 서로 다른 프로그램을 실행할 수는 없다는 것이다. 따라서 스레드를 통해 병렬성을 얻을 수는 있지만 이것은 제한되어 있다. 실제로 많은 사람들은 관련되지 않은 서로 다른 프로그램들을 동시에 실행하기를 원하며 이것은 스레드 추상화가 지원하지 않는 기능이다. 

메모리 추상화가 없는 환경에서 여러 프로그램 실행
------
　메모리 추상화가 없는 시스템에서도 여러 프로그램을 동시에 실행하는 방법이 가능하긴 하다. 운영체제가 해야하는 일은 우선 메모리에 존재하던 프로그램 이미지를 디스크에 저장하고 다음에 실행할 프로그램을 메모리로 올리는 것이다. 사실상 메모리에 한 순간에 하나의 프로그램이 존재하기만 하면 충돌을 방지할 수 있다.<br/>
　특별한 하드웨어의 도움이 있으면 스와핑을 사용하지 않더라도 여러 프로그램을 동시에 실행하는 것이 가능하다. **보호 키(protection key)**를 통해 사용자 프로세스들은 서로 간섭하는 것을 예방할 수 있으며 운영체제 역시 프로세스들로부터 간섭을 막아 보호할 수 있다. 하지만 예를 들어 프로그램 2개가 서로 다른 보호키를 가지고 있을 때 두 프로그램들이 모두 절대 물리 주소(absolute physical address)를 사용할 경우 문제가 발생한다.<br/>
　이를 해결하기 위해 **정적 재배치(static relocation)**라는 기법을 사용한다. 정적 재배치란 프로그램이 메모리에 적재될 때 프로그램의 내용을 수정하는 것이다. 하지만 이 기법은 일반적인 방법은 아니며 또한 적재하는 시간을 증가시킨다. 더욱이 이 기법은 프로그램의 어느 위치에 주소가 있는지 부가적인 정보를 요구한다.<br/>

메모리 추상화: 주소 공간
======
　프로세스가 물리 메모리를 직접 사용하는 것은 다음과 같은 단점이 있다.
* 프로그램이 물리 메모리의 모든 주소를 접근할 수 있다면 사용자는 실수로 또는 의도적으로 운영체제를 파괴할 수 있으며 결국 시스템을 중지시킬 수 있다.
* 여러 프로그램들을 동시에 실행시키는 것이 어려워진다.

주소 공간 개념
------
　여러 프로그램들을 동시에 메모리에 적재하고 서로 간섭 없이 실행하기 위해서는 **보호(protection)**와 **재배치(relocation)** 방법이 제공되어야 한다. IBM360에서는 메모리 공간마다 보호 키를 연결하고 프로세스가 메모리를 접근할 때 키를 비교하는 벙법으로 보호를 제공한다. 한편, 이것만으로는 재배치를 제공하지 못하기 때문에 프로그램을 적재할 때 프로그램이 참조하는 주소를 직접 바꾸는 방법으로 재배치를 제공하였다. 하지만 이 방법은 느리고 복잡하다.<br/>
　보호와 재배치를 제공하는 더 효과적인 방법은 **주소 공간(address space)**이라는 새로운 메모리 추상화를 제공하는 것이다. 주소 공간이란 프로세스가 메모리를 접근할 때 사용하는 주소들의 집합으로 각 프로세스는 자신만의 주소 공간을 갖는다. 한 프로세스가 갖는 주소 공간은 다른 프로세스가 갖는 주소 공간과 독립되어 있다. 각 프로세스들에게 서로 다른 주소 공간을 제공해야 하는데, 한 프로그램에서 사용하는 28 주소가 다른 프로그램에서 사용하는 28 주소와는 서로 다른 물리 주소를 가리키도록 해야한다.<br/>
**Base와 Limit 레지스터**<br/>
　각 프로세스의 주소 공간을 물리 메모리의 서로 다른 주소 공간으로 연속적으로 매핑하는 방법은 **동적 재배치(dynamic relocation)** 방법 중의 한가지로 상당히 단순하다. 이 방법을 위해 CPU는 **base**와 **limit**이라는 이름의 특별한 하드웨어 레지스터를 사용한다. 프로그램이 실행될 때 base 레지스터에 프로그램이 적재된 메모리 시작 위치가, limit 레지스터에 프로그램의 크기가 저장된다. 예를 들어, 첫 번째 프로그램이 실행될 때 base와 limit에는 각각 0과 16,384가 저장되며, 두 번째 프로그램에는 16,384와 16.384가 저장된다.<br/>
　프로세스가 명령어 반입이나 데이터를 읽고 쓰기 위해 메모리를 참조하면 CPU 하드웨어는 자동으로 프로세스가 참조하려는 메모리 주소에 base 레지스터 값을 더한다. 또한 프로세스가 참조하려는 주소가 limit 레지스터의 값과 동일한지 혹은 큰지를 확인한다. 만일 그렇다면 메모리 참조는 중단되고 결함(fault)이 발생한다. 아니라면 더한 값을 참조하려는 메모리 주소 값을 메모리 버스에 보낸다.<br/>
　Base와 limit 레지스터는 각 프로세스에게 자신 고유의 주소 공간을 제공하는 간단한 방법을 지원한다. 참조되는 모든 메모리 주소는 명령이 접근하려는 주소와 base 레지스터에 기록된 값이 더해진 값이 되며 이것은 하드웨어에 의해 자동으로 계산된다. 많은 구현에서 base와 limit 레지스터는 오직 운영체제만 변경할 수 있도록 보호된다.<br/>
　Base와 limit 레지스터를 사용하는 재배치의 단점은 모든 메모리 참조마다 덧셈과 비교 연산이 요구 된다는 것이다. 비교 연산은 상대적으로 빠르게 처리될 수 있지만 덧셈은 특별한 하드웨어 로직을 사용하지 않으면 캐리 전파 때문에 시간이 많이 걸린다.<br/>

스와핑
------
　지금까지는 시스템에 존재하는 물리 메모리의 용량이 모든 프로세스를 적재할 만큼 충분히 많은 경우에 대한 경우를 보았다. 하지만 실제 시스템에서 모든 프로세스들이 필요로 하는 메모리의 전체 크기는 시스템에 존재하는 실제의 RAM 용량보다 크다. 이러한 문제를 해결하기 위한 방법으로 **스와핑(swapping)**과 **가상 메모리(virtual memory)**가 제안되었다. 스와핑은 한 프로세스의 모든 이미지가 메모리로 적재되어 실행되다가 더 이상 실행되지 않을 경우 다시 디스크로 내려 보내는 방법이다. 따라서 현재 실행되고 있지 않은 프로세스는 메모리를 차지하지 않고 디스크에 존재하게 된다. 주기적으로 깨어나 실행하고 다시 수면에 빠지는 프로세스들은 메모리의 상태에 따라 계속 메모리에 존재할 수도 있고, 또는 실행 중일 때만 메모리에 존재하고 수면에 들어가면 다시 스와핑 될 수도 있다. 가상 메모리는 한 프로세스의 전체 이미지가 아닌 일부만 메모리에 있어도 그 프로세스의 실행이 가능하다.<br/>
　다음은 스와핑 시스템의 동작을 에시한 것이다.<br/>
<p align="center"><img src="/assets/img/Operating-Systems/3장-메모리-관리/2-1.png" width="600"></p>

　초기에는 메모리에 프로세스 A만 존재한다. 그러다가 프로세스 B와 C가 새로 생성 되었거나 디스크에서 스왑 인(swap in) 되었다. (d)는 프로세스 A가 스왑 아웃(swap out) 된 것을 보여주며, 그 이후에 프로세스 D가 적재 죄었고, 프로세스 B가 스왑 아웃되었다. 마지막으로 프로세스 A가 스왑 인 되었다. 마지막 상태에서 프로세스 A의 메모리 위치가 바뀌었기 때문에 재배치가 필요하다. 재배치는 스왑 인 될 때 소프트웨어적으로 실행될 수도 있으며, 또는 프로그램이 실행될 때 하드웨어적으로 실행될 수도 있다.<br/>
　스와핑 결과 메모리에 여러 개의 분리된 빈 공간들이 만들어지며 프로세스들의 위치를 이동하여 빈 공간들을 모아 하나의 큰 공간으로 합칠 수 있다. 이것을 **메모리 조각모음(memory compaction)**이라고 한다. 메모리 조각모음은 시간이 많이 걸리는 작업이기 때문에 자주 실행되지는 않는다.<br/>
　만약 프로세스의 크기가 실행 중에 증가할 것으로 예상된다면 프로세스가 생성되거나 스왑 인 될 때 여분의 빈 공간을 더 할당해 주는 것도 좋은 생각이다. 이것은 프로세스의 크기 증가에 따라 프로세스를 이동시키거나 다른 프로세스들을 스왑 아웃 시키는 부하를 줄일 수 있다. 다음 (a)는 공간 확장을 고려하여 두 프로세스에게 여분의 공간을 더 할당한 예를 보여준다. 이때 이 프로세스를 디스크로 스왑 아웃 한다면 실제 사용하는 메모리 내용만 스왑 아웃 하면 된다. 여분의 메모리 내용까지 스왑 아웃한다면 사실상 스왑 공간을 낭비하는 것이 된다.<br/>
<p align="center"><img src="/assets/img/Operating-Systems/3장-메모리-관리/2-2.png" width="600"></p>

　메모리의 동적 할당과 해제가 가능한 힙을 위한 데이터 세그먼트와 지역 변수 및 복귀 주소가 저장되는 스택 세그먼트라는 두 개의 세그먼트를 갖는 첫처럼 만일 프로세스가 증가될 수 있는 두 개의 세그먼트를 갖는다면 (b)와 같은 메모리 배치가 가능하다. 이 그림에서 각 프로세스는 할당 받은 메모리 상단에 스택 세그먼트를 배치하고 데이터 세그먼트는 메모리 하단의 프로그램 텍스트 위에 배치한다. 스택은 아래방향으로 자라며 데이터는 위 방향으로 자란다. 중간의 메모리 공간은 스택 또는 힙 어느 공간으로도 사용될 수 없다. 만일 메모리가 부족하다면 다른 여분의 공간이 큰 메모리 위치로 이동하거나, 스왑 아웃 되거나, 강제 종료된다.

가용 메모리 공간 관리
------
　메모리가 동적으로 할당된다면 운영체제는 메모리 공간의 어떤 부분이 사용 중인지 관리하여야 한다. 메모리 사용량을 관리할 때 사용하는 대표적인 두 가지 자료 구조가 **비트맵(bit map)**과 **리스트(list)**이다.<br/>

**비트맵을 이용한 관리**<br/>
　비트맵을 이용한 관리 방법은 메모리를 여러 개의 할당 단위(allocarion unit)로 나누어 관리한다. 할당 단위는 몇 워드 크기를 갖는 작은 단위에서부터 몇 KB크기를 갖는 큰 단위까지 가능하다. 각 할당 단위마다 비트가 하나씩 대응되는데, 이 비트가 0이면 해당 할당 단위가 가용하고 1이면 이미 사용 중임을 의미한다.(반대 설정도 가능) 다음은 메모리의 일부분과 이를 관리하는 비트맵의 예를 보여준다.<br/>
<p align="center"><img src="/assets/img/Operating-Systems/3장-메모리-관리/2-3.png" width="600"></p>

　할당 단위의 크기 설정은 중요한 설계 이슈이다.
* 할당 단위의 크기가 작으면 비트맵 공간이 커진다.
  - 4바이트를 할당 단위로 설정한다 하더라도 4바이트마다 1비트만 존재하면 된다. 즉, 32비트마다 1비트가 필요한 것이므로 32n 비트에 n 크기의 비트맵이 필요하게 되며 결국 전체 메모리의 1/33이 비트맵으로 사용된다.
* 반면 할당 단위가 커지면 비트맵의 공간이 작아진다.
  - 이 경우에는 프로세스의 크기가 할당 단위의 정수배가 아니면 마지막 할당 단위의 일부 공간이 낭비된다.

　비트맵 크기는 메모리 크기와 할당 단위의 크기에 의해 결정되며, 고정된 크기의 공간으로 메모리의 사용량을 관리하는 간단하면서도 효과적인 방법이다. 하지만 비트맵 검색은 시간이 많이 걸린다.<br/>

**연결 리스트를 이용한 관리**<br/>
　메모리의 사용량을 관리하는 다른 방법은 할당된(allocated) 메모리 공간과 가용한(free) 메모리 공간을 연결 리스트(linked list)로 관리하는 것이다. 위 그림에서 (a)의 메모리 상태를 연결 리스트로 나타낸 것이 (c)이다. 리스트의 각 엔트리는 빈 공간(H)이거나 프로세스의 내용(P)을 담고 있음을 나타내는 정보, 시작하는 주소, 길이, 그리고 다음 엔트리를 가리키는 포인터로 구성된다.<br/>
　이 예에서 리스트의 각 엔트리들은 시작 주소를 키로 정렬되어 있다. 이러한 방식의 정렬은 프로세스가 종료되거나 스왑 아웃될 때 리스트의 관리를 쉽게 한다. 종료되는 프로세스는 메모리의 가장 끝에 존재하는 프로세스를 제외하면 일반적으로 2개의 이웃을 갖는다. 이웃은 다른 프로세스가 차지한 공간이거나 빈 공간이며, 다음과 같이 4가지 조합이 가능하다.<br/>
<p align="center"><img src="/assets/img/Operating-Systems/3장-메모리-관리/2-4.png" width="600"></p>

　프로세스 X가 종료될 때 가능한 4가지 이웃들의 조합
* (a): 리스트의 반경은 해당 엔트리의 P를 H로 바꾸는 것 만으로 완료된다.
* (b), (c): 두 개의 엔트리가 통합되어 하나로 되며, 결국 전체 리스트에서 엔트리가 하나 줄어 든다.
* (d): 3개의 엔트리가 하나로 통합되며, 결국 리스트 전체적으로 2개의 엔트리가 줄어 든다.

　프로세스가 사용 중인 메모리 공간과 빈 공간들이 주소 값을 키로 정렬되어 있으면 새로 생성되는 프로세스 또는 디스크에서 스왑 인 되는 프로세스를 위한 메모리 공간을 할당할 때 다양한 알고리즘을 적용할 수 있다. 일단 메모리 관리자가 얼마 크기의 메모리 공간을 할당해야 하는지 이미 알고 있다고 가정하자.
* **최초적합(first fit)**
  - 가장 간단한 알고리즘.
  - 메모리 관리자가 리스트를 순서대로 검색하며 요청한 공간을 담을 수 있는 크기의 빈 공간이 발견되면 그 공간을 할당한다.
  - 최초적합은 검색을 최소한으로 하기 때문에 빠른 할당이 가능하다.
* **다음적합(next fit)**
  - 최초적합을 약간 수정한 알고리즘. 최초적합과 유사하게 동작한다.
  - 자신이 빈 공간을 할당해 주었을 때 그 위치를 기억해둔다. 새로운 할당을 위해 알고리즘을 다시 호출되면 지난 번에 기억해 두었던 위치부터 검색을 시작한다.(최초적합은 항상 리스트의 처음부터 검색을 시작)
  - 최초적합에 비해 성능이 조금 떨어짐
* **최적적합(best fit)**
  - 리스트의 처음부터 끝까지 모든 엔트리들을 검색하여 요청한 크기에 가장 근접하게 큰 빈 공간을 할당한다.
  - 큰 빈 공간을 잘라서 할당하는 것이 아니라 가능한 요청한 크기를 최적으로 만족할 수 있는 빈 공간을 찾는 것이다.
* **최악적합(worst fit)**
  - 가능한 프로세스가 요구하는 크기에 맞도록 빈 공간을 할당하고 그 결과를 작은 빈 공간들을 많이 생성하는 문제를 해결.
  - 항상 가장 큰 빈 공간을 할당해 준다. 그 결과 할당되고 남은 빈 공간도 가능한 큰 공간이 될 수 있도록 하는 것이다.

가상 메모리
======
　메모리 크기보다 큰 프로그램의 실행 문제가 존재한다. 이 해결책으로 **오버레이(overlay)**라고 불리는 작은 조각들로 나누어 실행하는 방법이 있다. 이 방법은 오버레이 관리자를 메모리로 적재한다. 이 관리자는 오버레이 0을 메모리로 올려 실행한다. 오버레이 0의 실행이 끝나면 관리자는 오버레이 1을 적재하여 실행한다. 오버레이 1은 오버레이 0과 다른 위치에 적재될 수도 있으며(가용 메모레 공간이 있는 경우), 오버레이 0이 존재하던 위치에 덮어 적재될 수도 있다(가용 메모리 공간이 없는 경우).<br/>
　**가상 메모리(virtual memory)**라고 알려진 기법의 기본 아이디어는 각 프로그램이 자신의 고유한 주소 공간을 가지며 주소 공간은 **페이지(page)**라고 불리는 조각들로 구성된다. 각 페이지는 연속된 주소를 갖는다. 프로그램이 실행되면 페이지들은 물리 메모리에 매핑된다.<br/>

페이징(paging)
------
　프로그램은 실행되면서 메모리 주소들을 참조한다. 에를 들어 "MOV REG, 1000"을 실행하면 메모리 주소 1000번지에 있는 내용을 REG라는 레지스터로 복사하는 일을 한다.<br/>
　프로그램이 참조하는 주소는 **가상 주소(virtual address)**라고 불리며 가상 주소 공간을 형성한다. 가상 메모리를 사용하지 않은 컴퓨터에서는 가상 주소가 그대로 메모리 버스에 실리며 결국 이 주소가 물리 주소(physical address)가 된다. 따라서 이 주소에 존재하는 데이터를 접근할 수 있다. 반면 가상 메모리를 사용하는 시스템에서는 가상 주소가 그대로 메모리 버스에 실리지는 않는다. 그 대신 가상 주소는 다음과 같이 **MMU(memory Management Unit)**에 의해 물리 주소로 매핑된다.
<p align="center"><img src="/assets/img/Operating-Systems/3장-메모리-관리/3-1.png" width="600"></p>

　다음은 매핑이 어떻게 동작하는지 예를 보여준다.<br/>
<p align="center"><img src="/assets/img/Operating-Systems/3장-메모리-관리/3-2.png" width="400"></p>

　위 예에서 컴퓨터는 16비트의 주소, 즉 0~64KB의 주소공간을 제공한다. 이 주소 공간은 가상 주소이다. 이 컴퓨터의 물리 메모리 크기는 32KB이다. 따라서 64KB 크기의 프로그램은 실행 중에 자신의 모든 페이지들을 물리 메모리 상에 적재할 수는 없다. 프로그램의 전체 이미지는 디스크 상에 존재하며 물리 메모리에는 현재 실행에 필요한 부분만 존재하게 된다.<br/>
　가상 주소 공간은 고정된 크기의 단위들로 구분되어 있으며 이 단위를 **페이지(page)**라고 부른다. 반면, 물리 메모리 상에 대응되는 단위는 **페이지 프레임(page prame)**이라고 부른다. 페이지와 페이지 프레임의 크기는 같다. 위 예에서는 각각 4KB 크기이며 64KB의 가상 메모리는 16개의 페이지로 구분되며, 32KB의 물리 메모리는 8개의 페이지 프레임으로 구분된다. RAM과 디스크 간에 데이터의 이동 역시 페이지 단위로 이루어진다.<br/>
　MMU가 16개의 가상 페이지를 8개의 페이지 프레임에 매핑하는 것 차제 만으로는 가상 주소 공간이 물리 주소보다 큰 문제를 해결하지 못한다. 이를 해결하기 위해서는 다른 기능들이 추가되어야 한다. 위 그림의 예에서는 페이지 프레임이 8개 밖에 없기 때문에 16개의 가상 페이지 중에서 오직 8개만 물리 메모리로 매핑될 수 있다. 그림에서 x로 표시된 페이지들이 매핑되지 목한 페이지들이다. 이를 위해 하드웨어적으로 **present/absent** 비트가 제공되며, 이를 통해 어떤 페이지가 실제 물리 메모리에 존재하는지 파악할 수 있다.<br/>
　MMU는 비트를 이용해 이 페이지가 매핑되어 있지 않음을 파악하고, CPU에게 트랩을 발생시켜 운영체제에게 이것은 알리도록 한다. 이것을 **페이지 폴트(page fault)**라고 한다. 운영체제는 페이지 프레임 중에서 적게 사용되고 있는 페이지 프레임을 선택하고 페이지 프레임의 내용을 디스크에 기록한다(이미 기록되어 있다면 생략). 그 이후 참조하려는 페이지의 내용을 페이지 프레임에 적재하고, 맵을 수정한 후, 트랩을 야기한 명령을 다시 실행한다.<br/>
　예를 들어 운영체제가 위의 페이지 프레임 중에서 1번을 교체하기로 선택했다고 가정하자. 그럼 운영체제는 페이지 8의 내용을 페이지 프레임 1에 적재하고 MMU에서 다음 두 가지를 수행한다. 첫 번째는 수정은 가상 페이지 1이 이제는 매핑되지 않았음을 표시한다. 이렇게 되면 이제부터 가상 주소 4096~8191의 참조는 트랩을 야기한다. 두 번째 수정은 가상 페이지 8번이 페이지 프레임 1에 매핑되었음을 기록한다. 그러면 트랩이 발생한 위 명령을 다시 실행할 때 이제는 가상 주소 32780이 물리 주소 4108(4096 + 12)으로 변환되어 실제 물리 메모리를 참조하게 된다.<br/>
　MMU의 내부 구조를 살펴보면서 동작 방법과 페이지 크기를 2의 정수 배로 선택한 이유를 보자. 다음 그림(4KB 크기의 페이지들을 16개로 관리하는 MMU의 내부 동작)은 가상 주소 8196(0010 0000 0000 0100)이 위 그림에서 예시된 매핑 정보를 이용해 물리 주소로 전환되는 과정을 보여준다.<br/>
<p align="center"><img src="/assets/img/Operating-Systems/3장-메모리-관리/3-3.jpg" width="600"></p>

　16비트 크기를 갖는 가상 주소는 페이지 번호와 오프셋으로 구분된다. 이 예에서는 페이지 번호(virtual page number)를 위해 4비트가 사용되고(페이지 개수가 16개), 오프셋(vitrual page offset)을 위해 12비트가 사용된다(페이지의 크기가 4096).<br/>
　페이지 번호는 **페이지 테이블(page table)**의 인덱스로 사용된다. 페이지 테이블에는 해당 페이지 번호에 대응되는 페이지 프레임 번호가 기록되어 있다. 만일 _present/absent_ 비트가 0이면 페이지 폴트가 발생한다. 반면 1이라면 페이지 테이블에 기록되어 있는 페이지 프레임 번호(pageframe number) 세 비트와 가상 주소의 오프셋에 대응되는 12비트가 결합하여 물리 주소가 된다. 이 주소는 주소 출력 레지스터를 통해 메모리 버스로 전달되고, 결국 메모리 참조 주소가 된다.<br/>

페이지 테이블
------
　가상 주소를 물리 주소로 매핑하는 과정을 요약하면 다음과 같다.
* 가상 주소를 페이지 번호(상위 비트)와 오프셋(하위 비트)로 구분한다.
  - 예를 들어 16비트 주소 크기를 갖는 시스템에서 페이지 크기가 4KB라면, 상위 4비트는 페이지를 가리키는 페이지 번호로 사용되고, 하위 12비트는 바이트 오프셋(0부터 4095까지)으로 사용된다.
  - 물론 페이지 번호로 3비트, 5비트 또는 다른 개수의 비트를 사용하는 것도 가능하지만 페이지의 크기도 변하게 된다.
* 페이지 번호를 인덱스로 이용해 페이지 테이블에서 가상 주소에 대응되는 앤트리를 찾는다.
  - 페이지 테이블 앤트리에서 페이지 프레임 번호를 얻을 수 있다(매핑되어 있다면).
* 가상 주소에서 페이지 번호가 차지하던 부분을 페이지 프레임 번호로 대치하면(즉, 페이지 프레임 번호를 상위에 놓고, 오프셋을 하위에 놓으면) 결국 물리주소가 된다.
  - 이 물리 주소가 메모리 참조에 사용된다.

　결국 페이지 테이블의 목표는 가상 페이지를 물리 페이지 프레임으로 매핑하는 것이다. 이 결과를 이용하여 가상 주소의 페이지 번호 부분을 페이지 프레임 번호로 변경하면 물리 주소를 구하게 된다.<br/>
**페이지 테이블 엔트리 구조**<br/>

페이징 속도 향상
------
　페이지 테이블에는 두 개의 주요 이슈갸 있다.
* The page table can be extremely large
  - 최근 컴퓨터는 가상 주소를 위해 32비트를 사용하며 64비트를 사용하는 경우도 많아지고 있다. 따라서 페이지 테이블의 크기도 고려해야 한다.
  - 예를 들어 만일 4KB 크기의 페이지를 사용한다면 32비트 시스템에서는 백 만개의 페이지가 필요하며($\frac{2^{32}}{2^{12}(= 4\mathrm{KB})} = 2^{20} = 100만$ ) 64비트 시스템에서는 우리가 상상한 것보다 훨씬 많은 페이지가 존재하게 된다. 백 만개의 페이지가 존재하기 위해서는 백 만개의 페이지 테이블 엔트리가 필요하다.
  - 각 프로세스마다 자신 고유의 주소 공간을 갖기 때문에 각 프로세스마다 자신의 고유한 페이지 테이블을 갖는다.
* The mapping must be fast
  - 크고 빠른 페이지 매핑의 요구는 컴퓨터 시스템을 구현하는데 크게 영향을 주었다.
  - 가장 간단한 구현 방법은 빠른 하드웨어 레지스터 배열로 구성된 단일한 페이지 테이블을 사용하는 것이다.
  - 이 테이블의 각 엔트리는 각각 가상 페이지에 대응되며 가상 페이지 번호에 의해 인덱싱 된다.
<br/>

 **Translation Lookaside Buffers**<br/>

 대용량 메모리를 위한 페이지 테이블
 ------

 s